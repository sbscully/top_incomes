#!/usr/bin/env perl

use strict;
use warnings;

use feature 'say';
use Data::Dumper;
sub inspect { say Dumper @_ }

use JSON::XS;
use File::Slurp;

sub compose {
  my $g = pop;
  my $f = pop;
  my @subs = @_;

  if ( @subs ) {
    sub { $g->( compose(@subs, $f)->(@_) ) }
  }
  else {
    sub { $g->($f->(@_)) };
  }
}

sub pipeline (+@) {
  my ($args, @subs) = @_;

  @subs = map { ref $_ eq 'CODE' ? $_ : \&$_ } @subs;

  compose(@subs)->(@$args)
}

sub transpose {
  my $data = \@_;
  my $inverted = [];

  foreach my $i ( (0..scalar(@$data)-1) ) {
    foreach my $j ( (0..scalar(@{ $data->[1] })-1) ) {
      $inverted->[$j] //= [];
      $inverted->[$j][$i] = $data->[$i][$j];
    }
  }

  return @$inverted;
}

sub zip { my ($a, $b) = @_; map { [ $a->[$_], $b->[$_] ] } (0..scalar(@$a)-1) }

# Takes an array of pairs and filters out a pair if either
# of the elements is undefined
sub compact_pairs { grep { defined $_->[0] && defined $_->[1] } @_ }

sub flatten { map { ref $_ eq 'ARRAY' ? flatten(@$_) : $_ } @_ }

sub hashref { my %hash = @_; \%hash }

# Derefere  nce if passed a lone hashref or arrayref
sub deref {
  my $ref = ref $_[0];
  my $single = scalar(@_) eq 1;

  if    ( $ref eq 'ARRAY' && $single ) { @{ $_[0] }; }
  elsif ( $ref eq 'HASH' && $single )  { %{ $_[0] }; }
  else                                 { @_; }
}

# A wrapper around hash key access to allow it to be pipelined
sub fetch { my ($key) = @_; sub { my %hash = deref @_; deref $hash{$key} } }

# A wrapper around array access to allow it to be pipelined
sub at { my ($index) = @_; sub { my @array = deref @_; deref $array[$index] } }

################################################################

{
  my $in = 'data/converted.json';
  my $out = 'data/transformed.json';

  if (-f $out) { unlink $out }

  my $json = pretty_encode_json([ transform_top_incomes($in) ]);
  write_file $out, $json;
}

# Take the Spreadsheet::Read output of the "Capital in the
# 21st Century" (Pikkety) xlsx file and extract the relevant data.
#
# The table is transposed so that each row corresponds to a set
# of data points for one country for one year.
#
#   {
#      "Number of tax units-married couples &amp; single adults" : "22127.693",
#      "Income control" : "1682.08332",
#      "Price index" : "0.0101303",
#      "Average income per tax unit-married couples &amp; single adults" : "7503.9",
#      "Top 0.05-0.01% income share-married couples &amp; single adults" : "4.18",
#      "Year" : "1908.0",
#      "Country" : "United Kingdom",
#      "Top 0.05% income share-married couples &amp; single adults" : "8.22",
#      "Top 0.01% average income-married couples &amp; single adults" : "3031574.98",
#      "Top 0.01% income share-married couples &amp; single adults" : "4.04"
#   }
#
# The first two rows are skipped because they are just id fields
# that we are not interested in.
#
# Finally the data is transformed from a table into an array of
# hashrefs.
sub transform_top_incomes {
  say "parsing input...";
  my @table = pipeline @_,
    'read_file',
    'decode_json',
    at(1),
    fetch('cell'),
    'transpose',
    sub { splice @_, 2 };

  my $count = scalar @table;
  say "transforming $count rows...";

  my $headers = shift @table;
  map { row_to_hashref($headers, $_) } @table;
}

# Take table headers and a row, and turn them into a compact hashref
# with the table headers as keys.
sub row_to_hashref { pipeline @_, qw(zip compact_pairs flatten hashref) }

sub pretty_encode_json { JSON::XS->new->utf8->pretty(1)->encode(@_) }